{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment setup\n",
    "\n",
    "from io import BytesIO\n",
    "from pprint import pprint\n",
    "import ollama\n",
    "import PIL.Image\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# llama3 models\n",
    "LLAMA_32_VISION = 'llama3.2-vision:11b'\n",
    "LLAMA_32_3B = 'llama3.2:3b'\n",
    "LLAMA_31_8B = 'llama3.1:8b'\n",
    "\n",
    "# convert a PIL Image to bytes for use with llama 3.2\n",
    "def image_to_bytes(image: PIL.Image.Image) -> BytesIO:\n",
    "    image_bytes = BytesIO()\n",
    "    image.save(image_bytes, format='JPEG')\n",
    "    return image_bytes.getvalue()\n",
    "\n",
    "# print model response as formatted markdown\n",
    "def print_response(response_string):\n",
    "    display(Markdown(response_string))\n",
    "\n",
    "# ollama client\n",
    "OLLAMA_HOST = 'http://localhost:11434'\n",
    "llm = ollama.Client(host=OLLAMA_HOST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example output from a prompt\n",
    "\n",
    "response = llm.generate(model=LLAMA_31_8B,\n",
    "    prompt=\"Tell me about Chicago, Illinois\",\n",
    ")\n",
    "\n",
    "print_response(response['response'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example output based on image input\n",
    "\n",
    "input_image = PIL.Image.open('./example.jpg')\n",
    "display(input_image)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'tell me about this image',\n",
    "        'images': [image_to_bytes(input_image)],\n",
    "    }\n",
    "]\n",
    "\n",
    "response = llm.chat(model=LLAMA_32_VISION,\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "pprint(response)\n",
    "print_response(response.message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example conversation\n",
    "\n",
    "messages.append(response.message.model_dump())\n",
    "\n",
    "messages.append({\n",
    "    'role': 'user',\n",
    "    'content': 'describe the clothing of the woman in the image'\n",
    "})\n",
    "\n",
    "response = llm.chat(model=LLAMA_32_VISION,\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print_response(response.message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example embeddings\n",
    "\n",
    "response = llm.embed(model=LLAMA_32_3B,\n",
    "    input=['the quick brown fox jumped over the lazy dog', 'all work and no play makes jack a dull boy'],\n",
    ")\n",
    "\n",
    "print(len(response.embeddings))\n",
    "pprint(response.embeddings[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
