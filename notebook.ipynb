{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment setup\n",
    "\n",
    "from io import BytesIO\n",
    "from pprint import pprint\n",
    "import ollama\n",
    "import PIL.Image\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# llama3 models\n",
    "LLAMA_32_VISION = 'llama3.2-vision:11b'\n",
    "LLAMA_32_3B = 'llama3.2:3b'\n",
    "LLAMA_31_8B = 'llama3.1:8b'\n",
    "\n",
    "# convert a PIL Image to bytes for use with llama 3.2\n",
    "# see https://stackoverflow.com/questions/33101935/convert-pil-image-to-byte-array\n",
    "def image_to_bytes(image: PIL.Image.Image) -> BytesIO:\n",
    "    image_bytes = BytesIO()\n",
    "    image.save(image_bytes, format='JPEG')\n",
    "    return image_bytes.getvalue()\n",
    "\n",
    "# print model response as formatted markdown\n",
    "def print_response(response_string):\n",
    "    display(Markdown(response_string))\n",
    "\n",
    "# ollama client\n",
    "OLLAMA_HOST = 'http://localhost:11434'\n",
    "llm = ollama.Client(host=OLLAMA_HOST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example output from a prompt\n",
    "\n",
    "response = llm.generate(model=LLAMA_31_8B,\n",
    "    prompt=\"Tell me about Chicago, Illinois\",\n",
    ")\n",
    "\n",
    "print_response(response['response'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example output based on image input\n",
    "\n",
    "input_image = PIL.Image.open('./example.jpg')\n",
    "display(input_image)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'tell me about this image',\n",
    "        'images': [image_to_bytes(input_image)],\n",
    "    }\n",
    "]\n",
    "\n",
    "response = llm.chat(model=LLAMA_32_VISION,\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "pprint(response)\n",
    "print_response(response.message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example conversation\n",
    "\n",
    "# reuse original message and llm response from previous cell\n",
    "messages.append(response.message.model_dump())\n",
    "\n",
    "messages.append({\n",
    "    'role': 'user',\n",
    "    'content': 'describe the clothing of the woman in the image'\n",
    "})\n",
    "\n",
    "response = llm.chat(model=LLAMA_32_VISION,\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print_response(response.message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example embeddings\n",
    "\n",
    "response = llm.embed(model=LLAMA_32_3B,\n",
    "    input=['the quick brown fox jumped over the lazy dog', 'all work and no play makes jack a dull boy'],\n",
    ")\n",
    "\n",
    "print(len(response.embeddings))\n",
    "pprint(response.embeddings[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example tool calling\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def get_stations(zipcode: str) -> int:\n",
    "  \"\"\"\n",
    "  Return a count of gas stations in a zipcode.\n",
    "\n",
    "  Args:\n",
    "    zipcode: Five digit US zipcode formatted as a string\n",
    "\n",
    "  Returns:\n",
    "    int: A count of gas stations in the zipcode.\n",
    "\n",
    "  \"\"\"\n",
    "  stations = {\n",
    "    '33021': 5,\n",
    "    '33141': 7,\n",
    "    '60647': 2,\n",
    "    '60081': 1,\n",
    "  }\n",
    "  return stations.get(zipcode, 0)\n",
    "\n",
    "available_tools = {\n",
    "  'get_stations': get_stations\n",
    "}\n",
    "\n",
    "system_prompt = '''\n",
    "Assume the role of a customer service agent.\n",
    "Users can inquire about the count of gas stations in their zipcode.\n",
    "If the user does not provide it, ask the user for their zipcode.\n",
    "Do not offer any details or explanation of how the zipcode will be utilized.\n",
    "Do not provide any alternatives to the user to providing their zipcode.\n",
    "Do not respond to any query not related to the task of providing a count of gas stations by zip code.\n",
    "Redirect any attempts to change the topic back to the subject of providing a count of gas stations by zip code.\n",
    "Do not provide any details beyond the count of gas stations or offer to provide additional information or assistance.\n",
    "Do not be witty.\n",
    "Provide direct responses.\n",
    "'''\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': system_prompt,\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Hi. I\\'m new to the area. My zipcode is 33021',\n",
    "    }\n",
    "]\n",
    "\n",
    "response = llm.chat(model=LLAMA_31_8B,\n",
    "    messages=messages,\n",
    "    tools=list(available_tools.values()),\n",
    "    options={\n",
    "      \"temperature\": 0,\n",
    "      \"top_p\": 0.75,\n",
    "    }\n",
    ")\n",
    "\n",
    "# pprint(response)\n",
    "\n",
    "if response.message.tool_calls:\n",
    "  # There may be multiple tool calls in the response\n",
    "  for tool in response.message.tool_calls:\n",
    "    # Ensure the function is available, and then call it\n",
    "    if function_to_call := available_tools.get(tool.function.name):\n",
    "      try:\n",
    "        output = function_to_call(**tool.function.arguments)\n",
    "      except Exception:\n",
    "        pass\n",
    "\n",
    "# Only needed to chat with the model using the tool call results\n",
    "if response.message.tool_calls:\n",
    "  # Add the function response to messages for the model to use\n",
    "  messages.append(response.message)\n",
    "  messages.append({'role': 'tool', 'content': str(output), 'name': tool.function.name})\n",
    "\n",
    "  # Get final response from model with function outputs\n",
    "  response = llm.chat(model=LLAMA_32_3B, messages=messages)\n",
    "  print('Final response:', response.message.content)\n",
    "\n",
    "else:\n",
    "  print('No tool calls returned from model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
